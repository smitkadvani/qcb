{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fe14f-e758-4108-bc95-c0cb75361b7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1719275035.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ./../akita_utils/ import *\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import bioframe \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "import akita_utils\n",
    "import pysam\n",
    "import h5py\n",
    "genome_open = pysam.Fastafile('/project/fudenber_735/genomes/mm10/mm10.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0bf0d-860a-4b3c-9ef9-13dd4f319b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:28:37.678832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /spack/apps/linux-centos7-x86_64/gcc-8.3.0/python-3.9.2-uvcroioc4witkp6qf7mbebof5ix4wlb6/lib:/spack/apps/linux-centos7-x86_64/gcc-8.3.0/pmix-3.1.3-3sm6emyqaxapunh7rwbjvtaqoqe2e5z3/lib:/spack/apps/linux-centos7-x86_64/gcc-8.3.0/openmpi-4.0.2-ipm3dnvlbtxawpi4ifz7jma6jgr7mexq/lib:/spack/apps/linux-centos7-x86_64/gcc-8.3.0/openblas-0.3.8-2no6mfziiclwxb7lstxoos335gnhjpes/lib:/spack/apps/gcc/8.3.0/lib64\n",
      "2022-06-16 17:28:37.678878: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-16 17:28:37.678919: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d05-43.hpc.usc.edu): /proc/driver/nvidia/version does not exist\n",
      "2022-06-16 17:28:37.679213: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built\n",
      "restored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 130305, (130305,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import pysam\n",
    "from basenji import dataset, seqnn, dna_io,stream\n",
    "head_i = 1 #mouse\n",
    "model_num = 1 #which fold to use\n",
    "\n",
    "\n",
    "#base_dir = '/project/fudenber_735/backup/DNN_HiC/human-mouse_5-16-21/'\n",
    "#model_dir = base_dir+\"/f\"+str(model_num)+\"_c0/train/\"\n",
    "\n",
    "base_dir = '/project/fudenber_735/tensorflow_models/akita/v2/models/'\n",
    "model_dir = base_dir+\"/f\"+str(model_num)+\"c0/train/\"\n",
    "model_file  = model_dir+'/model'+str(head_i)+'_best.h5'\n",
    "\n",
    "\n",
    "# model_dir = '/home1/fudenber/repositories/basenji/manuscripts/akita/'\n",
    "# model_file = model_dir+'/model_best.h5'\n",
    "\n",
    "params_file = model_dir+'/params.json'\n",
    "with open(params_file) as params_open:\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "seq_length = params_model['seq_length']\n",
    "params_model['verbose'] = False\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n",
    "print('built')\n",
    "seqnn_model.restore(model_file, head_i=head_i)\n",
    "print('restored')\n",
    "\n",
    "hic_diags = params_model['diagonal_offset']\n",
    "try:\n",
    "    target_crop = params_model['trunk'][-2]['cropping']\n",
    "except:\n",
    "    target_crop = params_model['target_crop']\n",
    "target_length_cropped = int((seq_length//2048 - target_crop*2 - hic_diags)* ((seq_length//2048 - target_crop*2 - hic_diags) +1)/2) \n",
    "target_map_size = seq_length//2048  - target_crop*2 \n",
    "triu_tup = np.triu_indices(target_map_size,2)\n",
    "target_map_size, target_length_cropped, triu_tup[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be8af45-f450-4c1a-bb01-fa6913331d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_file = base_dir + '../analysis/background_seqs.fa'\n",
    "background_seqs = []\n",
    "with open(background_file,'r') as f:\n",
    "  for line in f.readlines():\n",
    "    if '>' in line: continue\n",
    "    background_seqs.append(dna_io.dna_1hot(line.strip())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763cd93f-a985-4e03-b40b-961459876a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig 26544 filt 26515\n",
      "29 duplicates removed\n",
      "orig 26544 filt 26515\n",
      "29 duplicates removed\n",
      "annotating each site with boundary-wide scores\n"
     ]
    }
   ],
   "source": [
    "seq_coords_df = akita_utils.prepare_insertion_tsv(\n",
    "    h5_dirs = '/project/fudenber_735/tensorflow_models/akita/v2/analysis/permute_boundaries_motifs_ctcf_mm10_model*/scd.h5',\n",
    "    score_key = 'SCD',\n",
    "    flank_pad = 60, #how much flanking sequence around the sites to include\n",
    "    weak_thresh_pct = 1, # don't use sites weaker than this, might be artifacts\n",
    "    weak_num = 10 ,\n",
    "    strong_thresh_pct = 99, # don't use sites weaker than this, might be artifacts\n",
    "    strong_num = 10 ,\n",
    "    save_tsv=None, # optional filename to save a tsv\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4a647a-ad15-4c28-8889-c611a2fb88ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([654943, 655082, 655221, 655360, 655499, 655638], 655360)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = seq_coords_df.iloc[0]\n",
    "seq_1hot_motif = dna_io.dna_1hot(genome_open.fetch(s.chrom, s.start, s.end).upper())\n",
    "insert_length = len(seq_1hot_motif)\n",
    "spacer_bp = 0\n",
    "num_inserts = 6\n",
    "multi_insert_length = num_inserts * (insert_length+spacer_bp)\n",
    "\n",
    "offsets = []\n",
    "for i in range(num_inserts):\n",
    "    offsets.append( seq_length//2 - multi_insert_length//2 + i * (insert_length+spacer_bp))\n",
    "offsets, seq_length//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5688533e-88c3-48f4-8536-7876746f7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inserts = []\n",
    "for s in seq_coords_df.itertuples():\n",
    "  for background_seq in background_seqs[0:1]:\n",
    "    seq_1hot = background_seq.copy()\n",
    "    seq_1hot_motif = dna_io.dna_1hot(genome_open.fetch(s.chrom, s.start, s.end).upper())\n",
    "    if s.strand == '-': seq_1hot_motif = dna_io.hot1_rc(seq_1hot_motif)\n",
    "    insert_length = len(seq_1hot_motif)\n",
    "    for offset in offsets:\n",
    "       seq_1hot[offset:offset+insert_length] = seq_1hot_motif\n",
    "    all_inserts.append(seq_1hot)  \n",
    "all_inserts = np.array( all_inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65642a78-e970-4e3f-ace9-aed9cd712ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:35:33.340295: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "## predict for each insertion\n",
    "pred = seqnn_model.predict(all_inserts, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a188fa4-6f37-4640-9f1f-c42fd4c2535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5*3,2*3))\n",
    "\n",
    "target_ind = 0\n",
    "vlim = .5\n",
    "bin_mid = target_map_size//2\n",
    "window = 50\n",
    "for i in range(len(seq_coords_df)):\n",
    "    insert_pred = pred[i]\n",
    "    print(i, np.sqrt( (insert_pred**2).sum(axis=0)))\n",
    "\n",
    "    plt.subplot(2,5, i+1)\n",
    "    im = plt.matshow(\n",
    "            from_upper_triu(  \n",
    "            insert_pred[:,target_ind], target_map_size,hic_diags),\n",
    "            vmin=-1*vlim, vmax=vlim, fignum=False,cmap='RdBu_r')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.title('genomic-scd: '+str(seq_coords_df['genomic_SCD'].values[i]) +'\\n'+\n",
    "              'insert-scd: '+str(  np.sqrt( (insert_pred**2).sum(axis=0)  ).mean() ) \n",
    "              ) \n",
    "    #plt.axis([ bin_mid  - window,bin_mid+window,bin_mid-window, bin_mid+window])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basenji kernel",
   "language": "python",
   "name": "basenji_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
